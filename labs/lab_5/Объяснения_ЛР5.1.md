# Объяснения к лабораторной работе 5.1: Логистическая регрессия

## Федоров Александр

## Общая структура работы

Лабораторная работа состоит из трех основных частей:
1. **Бинарная логистическая регрессия** - реализация класса с различными методами оптимизации и регуляризацией
2. **Применение на реальных данных** - демонстрация на датасете пингвинов с сравнением методов кодирования категориальных признаков
3. **Многоклассовая логистическая регрессия** - теоретический вывод и практическая реализация

---

## Часть 1: Бинарная логистическая регрессия

### Реализованный класс `MyBinaryLogisticRegression`

Класс поддерживает:
- **Типы регуляризации**: None, L1, L2, L1L2 (Elastic Net)
- **Методы оптимизации**: Градиентный спуск (GD), Стохастический градиентный спуск (SGD), Метод Ньютона

### Обоснование применимости методов оптимизации

#### Градиентный спуск (GD)
- ✅ **Применим для всех типов регуляризации**
- Для L1 регуляризации используется субградиент (знак функции в точке разрыва)
- Универсальный метод, работает стабильно

#### Стохастический градиентный спуск (SGD)
- ✅ **Применим для всех типов регуляризации**
- Эффективен для больших датасетов
- Использует случайные батчи, что ускоряет обучение
- Для L1 регуляризации также используется субградиент

#### Метод Ньютона
- ✅ **Применим только для L2 регуляризации или без регуляризации**
- ❌ **НЕ применим для L1 и L1L2 регуляризации**
- **Причина**: L1 регуляризация недифференцируема в точках, где веса равны нулю. Гессиан (матрица вторых производных) не существует в этих точках, что делает невозможным применение метода Ньютона, который требует вычисления обратного гессиана.

### Математические основы

**Функция потерь (бинарная кросс-энтропия):**
$$
L(w) = -\frac{1}{n}\sum_{i=1}^{n} [y_i \log(\sigma(w^T x_i)) + (1-y_i)\log(1-\sigma(w^T x_i))]
$$

где $\sigma(z) = \frac{1}{1+e^{-z}}$ - сигмоидная функция.

**Градиент:**
$$
\nabla_w L = \frac{1}{n}X^T(\sigma(Xw) - y) + \lambda \cdot \text{reg\_term}
$$

**Гессиан (для метода Ньютона):**
$$
H = \frac{1}{n}X^T S X
$$

где $S$ - диагональная матрица с элементами $\sigma(z_i)(1-\sigma(z_i))$.

---

## Часть 2: Применение на датасете пингвинов

### Подготовка данных

1. **Загрузка и очистка**: Удаление пропущенных значений
2. **Бинаризация целевой переменной**: Adelie → 0, Gentoo → 1
3. **Масштабирование числовых признаков**: StandardScaler для стабильности обучения

### Кодирование категориального признака `island`

#### One-Hot Encoding
- Создает отдельный бинарный признак для каждой категории
- Преимущества: простота, отсутствие утечки данных
- Недостатки: увеличение размерности, возможна мультиколлинеарность

#### Target Encoding (реализован самостоятельно)
- Кодирует категории средним значением целевой переменной по этой категории
- Используется сглаживание (smoothing) для предотвращения переобучения:
  $$
  \text{encoded\_value} = \frac{n \cdot \text{category\_mean} + \alpha \cdot \text{global\_mean}}{n + \alpha}
  $$
- Преимущества: сохраняет размерность, может улучшить качество модели
- Недостатки: риск утечки данных (нужно аккуратно применять на тестовой выборке)

### Результаты экспериментов

**Основные выводы:**
1. Все методы оптимизации показывают схожее качество на данном датасете
2. Метод Ньютона сходится быстрее (меньше итераций), но требует больше вычислений на каждой итерации
3. SGD эффективен для больших датасетов, но на малых может быть менее стабилен
4. Регуляризация (особенно L2) помогает предотвратить переобучение
5. Target Encoding показал сравнимые или лучшие результаты по сравнению с One-Hot Encoding

**Сравнение с sklearn:**
- Наша реализация показывает сопоставимые результаты с библиотечной реализацией
- Небольшие различия могут быть связаны с разными алгоритмами оптимизации и настройками параметров

**ROC-кривая и AUC:**
- ROC-кривая (Receiver Operating Characteristic) показывает зависимость между True Positive Rate и False Positive Rate при различных порогах классификации
- AUC (Area Under Curve) - площадь под ROC-кривой, метрика качества классификатора
- AUC = 1.0 означает идеальный классификатор, AUC = 0.5 - случайный классификатор
- Все реализованные модели показывают AUC > 0.9, что указывает на хорошее качество классификации

**Анализ важности признаков:**
- Коэффициенты логистической регрессии показывают влияние каждого признака на вероятность принадлежности к классу
- Положительные коэффициенты увеличивают вероятность класса 1 (Gentoo)
- Отрицательные коэффициенты увеличивают вероятность класса 0 (Adelie)
- Абсолютное значение коэффициента показывает силу влияния признака

---

## Часть 3: Многоклассовая логистическая регрессия

### Теоретический вывод

#### Функция потерь

Логарифмическое правдоподобие:
$$
\ell(\omega) = \sum_{i=1}^{N}\sum_{j=1}^{M} y_{ij} \ln \mathbb P(C_j|x_i)
$$

Подставляя формулу softmax:
$$
\ell(\omega) = \sum_{i=1}^{N}\sum_{j=1}^{M} y_{ij} \left( \omega_j^T x_i - \ln \sum_{k=1}^{M} \exp(\omega_k^T x_i) \right)
$$

Функция потерь (отрицательное логарифмическое правдоподобие):
$$
L(\omega) = -\sum_{i=1}^{N}\sum_{j=1}^{M} y_{ij} \left( \omega_j^T x_i - \ln \sum_{k=1}^{M} \exp(\omega_k^T x_i) \right)
$$

В матричной форме:
$$
L(W) = -\text{tr}(Y^T X W^T) + \sum_{i=1}^{N} \ln \sum_{k=1}^{M} \exp((W X^T)_{ki})
$$

#### Градиент

Градиент по весам класса $j$:
$$
\frac{\partial L}{\partial \omega_j} = -\sum_{i=1}^{N} x_i (y_{ij} - p_{ij})
$$

где $p_{ij} = \mathbb P(C_j|x_i) = \frac{\exp(\omega_j^T x_i)}{\sum_{k=1}^{M} \exp(\omega_k^T x_i)}$ - вероятность класса $j$ для объекта $i$.

В матричной форме:
$$
\nabla_W L = X^T (P - Y)
$$

где $P \in \mathbb{R}^{N \times M}$ - матрица вероятностей.

#### Гессиан

Гессиан по весам классов $j$ и $k$:
$$
\frac{\partial^2 L}{\partial \omega_j \partial \omega_k^T} = \sum_{i=1}^{N} x_i x_i^T p_{ij} (\delta_{jk} - p_{ik})
$$

где $\delta_{jk}$ - символ Кронекера.

Для класса $j$:
$$
H_j = \sum_{i=1}^{N} p_{ij} (1 - p_{ij}) x_i x_i^T
$$

### Реализация

Класс `MultiClassLogisticRegression`:
- Использует softmax для вычисления вероятностей классов
- Реализован градиентный спуск для оптимизации
- Матричные операции для эффективности вычислений

### Визуализация результатов

1. **Кривая обучения**: Показывает сходимость функции потерь
2. **Разделяющие поверхности**: Линейные границы между классами
3. **Карты вероятностей**: Визуализация вероятностей каждого класса в пространстве признаков

### Интерпретация гиперплоскостей

- **Границы решений**: Определяются условием равенства вероятностей двух классов
- Для классов $i$ и $j$ граница задается уравнением: $(\omega_i - \omega_j)^T x = 0$
- В 2D пространстве границы - прямые линии
- Каждая область соответствует классу с максимальной вероятностью

---

## Выводы

1. **Бинарная логистическая регрессия**: Успешно реализована с поддержкой различных методов оптимизации и регуляризации. Метод Ньютона не применим для L1 регуляризации из-за недифференцируемости.

2. **Кодирование категориальных признаков**: Target Encoding показал себя эффективной альтернативой One-Hot Encoding, сохраняя размерность и потенциально улучшая качество модели.

3. **Многоклассовая логистическая регрессия**: Теоретически обоснована и практически реализована. Матричная реализация обеспечивает эффективность вычислений.

4. **Визуализация**: Все графики и диаграммы помогают понять работу алгоритмов и интерпретировать результаты.

---

## Технические детали

- **Язык программирования**: Python 3
- **Основные библиотеки**: NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn
- **Метрики качества**: F1-score, Accuracy, Precision, Recall, Confusion Matrix, ROC-AUC
- **Визуализация**: 
  - Кривые обучения (loss по итерациям)
  - ROC-кривые с AUC для оценки качества классификации
  - Разделяющие поверхности и гиперплоскости
  - Карты вероятностей для каждого класса
  - Матрицы ошибок (confusion matrix)
  - Графики важности признаков (коэффициенты модели)
  - Сравнительные графики методов и регуляризации

